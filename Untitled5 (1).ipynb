{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 981
        },
        "id": "o7AyqClxhgrN",
        "outputId": "8ed91d99-c4cd-4f09-8d46-25472e2e6934"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.12.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "\n",
            "üîç Running evaluation for: gpt-4o-mini\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5 [00:05<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-184821016.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mMODEL_CONFIG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nüîç Running evaluation for: {model}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-184821016.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-184821016.py\u001b[0m in \u001b[0;36mrun_llm\u001b[0;34m(model, prompt)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1190\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1191\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1193\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ],
      "source": [
        "!pip install openai tiktoken pandas numpy tqdm scikit-learn\n",
        "\n",
        "import time\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tiktoken\n",
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "\n",
        "MODEL_CONFIG = {\n",
        "    \"gpt-4o-mini\": {\n",
        "        \"input_cost\": 0.15 / 1_000_000,\n",
        "        \"output_cost\": 0.60 / 1_000_000\n",
        "    },\n",
        "    \"gpt-4o\": {\n",
        "        \"input_cost\": 5.00 / 1_000_000,\n",
        "        \"output_cost\": 15.00 / 1_000_000\n",
        "    },\n",
        "    \"gpt-3.5-turbo\": {\n",
        "        \"input_cost\": 0.50 / 1_000_000,\n",
        "        \"output_cost\": 1.50 / 1_000_000\n",
        "    }\n",
        "}\n",
        "\n",
        "def count_tokens(text, model):\n",
        "    enc = tiktoken.encoding_for_model(model)\n",
        "    return len(enc.encode(text))\n",
        "\n",
        "evaluation_data = [\n",
        "    {\"question\": \"What is aspirin used for?\", \"answer\": \"pain relief\"},\n",
        "    {\"question\": \"What is diabetes?\", \"answer\": \"high blood sugar\"},\n",
        "    {\"question\": \"What does MRI stand for?\", \"answer\": \"magnetic resonance imaging\"},\n",
        "    {\"question\": \"What is hypertension?\", \"answer\": \"high blood pressure\"},\n",
        "    {\"question\": \"What is chemotherapy?\", \"answer\": \"cancer treatment\"},\n",
        "]\n",
        "\n",
        "def run_llm(model, prompt):\n",
        "    start = time.time()\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    latency = time.time() - start\n",
        "    output = response.choices[0].message.content.strip()\n",
        "\n",
        "    input_tokens = count_tokens(prompt, model)\n",
        "    output_tokens = count_tokens(output, model)\n",
        "\n",
        "    cost = (\n",
        "        input_tokens * MODEL_CONFIG[model][\"input_cost\"]\n",
        "        + output_tokens * MODEL_CONFIG[model][\"output_cost\"]\n",
        "    )\n",
        "\n",
        "    return output, latency, cost\n",
        "\n",
        "def evaluate_accuracy(preds, refs):\n",
        "    preds = [p.lower() for p in preds]\n",
        "    refs = [r.lower() for r in refs]\n",
        "    return accuracy_score(refs, preds)\n",
        "\n",
        "def run_experiment(model_name):\n",
        "    predictions, latencies, costs = [], [], []\n",
        "\n",
        "    for item in tqdm(evaluation_data):\n",
        "        output, latency, cost = run_llm(model_name, item[\"question\"])\n",
        "\n",
        "        predictions.append(output)\n",
        "        latencies.append(latency)\n",
        "        costs.append(cost)\n",
        "\n",
        "    accuracy = evaluate_accuracy(\n",
        "        predictions,\n",
        "        [x[\"answer\"] for x in evaluation_data]\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"model\": model_name,\n",
        "        \"accuracy\": accuracy,\n",
        "        \"avg_latency_sec\": np.mean(latencies),\n",
        "        \"avg_cost_usd\": np.mean(costs),\n",
        "        \"total_cost_usd\": np.sum(costs)\n",
        "    }\n",
        "\n",
        "results = []\n",
        "\n",
        "for model in MODEL_CONFIG.keys():\n",
        "    print(f\"\\nüîç Running evaluation for: {model}\")\n",
        "    metrics = run_experiment(model)\n",
        "    results.append(metrics)\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "df\n",
        "\n",
        "df.to_csv(\"llm_experiment_logs.csv\", index=False)\n",
        "print(\"‚úÖ Experiment logs saved\")\n",
        "\n",
        "best_model = df.sort_values(\n",
        "    by=[\"accuracy\", \"avg_cost_usd\", \"avg_latency_sec\"],\n",
        "    ascending=[False, True, True]\n",
        ").iloc[0]\n",
        "\n",
        "print(\"\\nüèÜ BEST MODEL SELECTED\")\n",
        "print(best_model)\n",
        "\n",
        "\n",
        "BASELINE_ACCURACY = 0.7\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    if row[\"accuracy\"] < BASELINE_ACCURACY:\n",
        "        print(f\"‚ö†Ô∏è Regression detected in model: {row['model']}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import RateLimitError\n",
        "import random\n",
        "\n",
        "def run_llm(model, prompt, max_retries=3):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            start = time.time()\n",
        "\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0\n",
        "            )\n",
        "\n",
        "            latency = time.time() - start\n",
        "            output = response.choices[0].message.content.strip()\n",
        "\n",
        "            input_tokens = count_tokens(prompt, model)\n",
        "            output_tokens = count_tokens(output, model)\n",
        "\n",
        "            cost = (\n",
        "                input_tokens * MODEL_CONFIG[model][\"input_cost\"]\n",
        "                + output_tokens * MODEL_CONFIG[model][\"output_cost\"]\n",
        "            )\n",
        "\n",
        "\n",
        "            time.sleep(1)\n",
        "\n",
        "            return output, latency, cost\n",
        "\n",
        "        except RateLimitError:\n",
        "            wait_time = 2 ** attempt + random.random()\n",
        "            print(f\"‚è≥ Rate limit hit. Retrying in {wait_time:.1f}s...\")\n",
        "            time.sleep(wait_time)\n",
        "\n",
        "\n",
        "    return \"ERROR\", 0, 0\n"
      ],
      "metadata": {
        "id": "fnMsON-BibCs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "JUDGE_PROMPT = \"\"\"\n",
        "You are an expert evaluator.\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Ground Truth Answer:\n",
        "{reference}\n",
        "\n",
        "Model Answer:\n",
        "{prediction}\n",
        "\n",
        "Score the model answer from 0 to 5:\n",
        "0 = completely wrong\n",
        "5 = perfectly correct\n",
        "\n",
        "Return ONLY a number between 0 and 5.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "MWBGJykWilI_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def judge_answer(question, reference, prediction, model=\"gpt-4o-mini\"):\n",
        "    prompt = JUDGE_PROMPT.format(\n",
        "        question=question,\n",
        "        reference=reference,\n",
        "        prediction=prediction\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0\n",
        "        )\n",
        "        score = float(response.choices[0].message.content.strip())\n",
        "        time.sleep(1)\n",
        "        return score\n",
        "    except:\n",
        "        return 0.0\n"
      ],
      "metadata": {
        "id": "rC8IB1VjioVp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "judge_scores = []\n",
        "\n",
        "for model in df[\"model\"]:\n",
        "    print(f\"\\nüßë‚Äç‚öñÔ∏è Judging answers for {model}\")\n",
        "    scores = []\n",
        "\n",
        "    for item in evaluation_data:\n",
        "        pred, _, _ = run_llm(model, item[\"question\"])\n",
        "        score = judge_answer(\n",
        "            item[\"question\"],\n",
        "            item[\"answer\"],\n",
        "            pred\n",
        "        )\n",
        "        scores.append(score)\n",
        "\n",
        "    judge_scores.append(np.mean(scores))\n"
      ],
      "metadata": {
        "id": "t1GmaWlpmZl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "judge_scores = []\n",
        "\n",
        "\n",
        "assert \"model\" in df.columns, \"df me 'model' column missing hai\"\n",
        "\n",
        "for model in df[\"model\"].tolist():\n",
        "    print(f\"\\nüßë‚Äç‚öñÔ∏è Judging answers for {model}\")\n",
        "    scores = []\n",
        "\n",
        "    for item in evaluation_data:\n",
        "        try:\n",
        "\n",
        "            pred, _, _ = run_llm(model, item[\"question\"])\n",
        "\n",
        "\n",
        "            if item[\"answer\"].lower() in pred.lower():\n",
        "                score = 5.0\n",
        "            elif pred.strip() != \"\":\n",
        "                score = 3.0\n",
        "            else:\n",
        "                score = 0.0\n",
        "\n",
        "            scores.append(score)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"‚ö†Ô∏è Error:\", e)\n",
        "            scores.append(0.0)\n",
        "\n",
        "    judge_scores.append(float(np.mean(scores)))\n",
        "\n",
        "print(\"\\n‚úÖ Judge scores calculated:\", judge_scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "XQEwn2jdmhEg",
        "outputId": "366885b6-2984-4dfc-fa92-9d899c581e8a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3964697291.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;34m\"model\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"df me 'model' column missing hai\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "for model in MODEL_CONFIG.keys():\n",
        "    print(f\"\\nüîç Running evaluation for: {model}\")\n",
        "    metrics = run_experiment(model)\n",
        "    results.append(metrics)\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cAzm0H_RmyNE",
        "outputId": "c726ba09-6c8a-4099-b4f8-dcffe7597782"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Running evaluation for: gpt-4o-mini\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Rate limit hit. Retrying in 1.1s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.5s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.9s...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|‚ñà‚ñà        | 1/5 [00:15<01:01, 15.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Rate limit hit. Retrying in 1.6s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.2s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.1s...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:28<00:42, 14.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Rate limit hit. Retrying in 1.7s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.4s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.7s...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:42<00:28, 14.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Rate limit hit. Retrying in 1.8s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.5s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.7s...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:56<00:13, 13.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Rate limit hit. Retrying in 1.7s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.0s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.1s...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:08<00:00, 13.71s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Running evaluation for: gpt-4o\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Rate limit hit. Retrying in 1.3s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.0s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.5s...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|‚ñà‚ñà        | 1/5 [00:12<00:48, 12.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Rate limit hit. Retrying in 1.9s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.0s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.9s...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:25<00:38, 12.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Rate limit hit. Retrying in 1.8s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.1s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.2s...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:38<00:25, 12.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Rate limit hit. Retrying in 1.9s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.7s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.1s...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:51<00:12, 12.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Rate limit hit. Retrying in 1.0s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.4s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.1s...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:03<00:00, 12.73s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Running evaluation for: gpt-3.5-turbo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Rate limit hit. Retrying in 1.9s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.9s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.0s...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|‚ñà‚ñà        | 1/5 [00:13<00:52, 13.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Rate limit hit. Retrying in 1.2s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.5s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.1s...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:25<00:37, 12.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Rate limit hit. Retrying in 1.7s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.5s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.0s...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:38<00:25, 12.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Rate limit hit. Retrying in 1.6s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.6s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.2s...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:51<00:12, 12.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Rate limit hit. Retrying in 1.1s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.6s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.8s...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:04<00:00, 12.86s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           model  accuracy  avg_latency_sec  avg_cost_usd  total_cost_usd\n",
              "0    gpt-4o-mini       0.0              0.0           0.0               0\n",
              "1         gpt-4o       0.0              0.0           0.0               0\n",
              "2  gpt-3.5-turbo       0.0              0.0           0.0               0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-287111a9-427c-489d-ac87-e883103fcec9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>avg_latency_sec</th>\n",
              "      <th>avg_cost_usd</th>\n",
              "      <th>total_cost_usd</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gpt-4o-mini</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gpt-4o</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-287111a9-427c-489d-ac87-e883103fcec9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-287111a9-427c-489d-ac87-e883103fcec9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-287111a9-427c-489d-ac87-e883103fcec9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-91e86e0b-b9fb-4119-9ad2-2eb0ef469379\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-91e86e0b-b9fb-4119-9ad2-2eb0ef469379')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-91e86e0b-b9fb-4119-9ad2-2eb0ef469379 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_d9a045d4-9a39-4c6a-852e-6ed8cc3d7b68\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_d9a045d4-9a39-4c6a-852e-6ed8cc3d7b68 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"gpt-4o-mini\",\n          \"gpt-4o\",\n          \"gpt-3.5-turbo\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_latency_sec\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_cost_usd\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_cost_usd\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XC4LdmJVm2iy",
        "outputId": "0450a851-e592-4a64-a7f1-3d2acad3bd1e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['model', 'accuracy', 'avg_latency_sec', 'avg_cost_usd',\n",
            "       'total_cost_usd'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "judge_scores = []\n",
        "\n",
        "for model in df[\"model\"].tolist():\n",
        "    print(f\"\\nüßë‚Äç‚öñÔ∏è Judging answers for {model}\")\n",
        "    scores = []\n",
        "\n",
        "    for item in evaluation_data:\n",
        "        pred, _, _ = run_llm(model, item[\"question\"])\n",
        "\n",
        "\n",
        "        if item[\"answer\"].lower() in pred.lower():\n",
        "            score = 5.0\n",
        "        elif pred.strip():\n",
        "            score = 3.0\n",
        "        else:\n",
        "            score = 0.0\n",
        "\n",
        "        scores.append(score)\n",
        "\n",
        "    judge_scores.append(float(np.mean(scores)))\n",
        "\n",
        "print(\"‚úÖ Judge scores:\", judge_scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdBry_ggnnbz",
        "outputId": "336bb64f-b4a3-4d73-b563-a2a7e1b31883"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üßë‚Äç‚öñÔ∏è Judging answers for gpt-4o-mini\n",
            "‚è≥ Rate limit hit. Retrying in 1.6s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.9s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.6s...\n",
            "‚è≥ Rate limit hit. Retrying in 1.5s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.3s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.5s...\n",
            "‚è≥ Rate limit hit. Retrying in 1.3s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.3s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.2s...\n",
            "‚è≥ Rate limit hit. Retrying in 1.9s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.5s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.2s...\n",
            "‚è≥ Rate limit hit. Retrying in 1.5s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.1s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.6s...\n",
            "\n",
            "üßë‚Äç‚öñÔ∏è Judging answers for gpt-4o\n",
            "‚è≥ Rate limit hit. Retrying in 1.4s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.4s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.5s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.0s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.2s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.6s...\n",
            "‚è≥ Rate limit hit. Retrying in 1.8s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.9s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.1s...\n",
            "‚è≥ Rate limit hit. Retrying in 1.6s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.6s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.2s...\n",
            "‚è≥ Rate limit hit. Retrying in 1.2s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.4s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.5s...\n",
            "\n",
            "üßë‚Äç‚öñÔ∏è Judging answers for gpt-3.5-turbo\n",
            "‚è≥ Rate limit hit. Retrying in 1.7s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.5s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.2s...\n",
            "‚è≥ Rate limit hit. Retrying in 1.5s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.1s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.9s...\n",
            "‚è≥ Rate limit hit. Retrying in 1.2s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.7s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.0s...\n",
            "‚è≥ Rate limit hit. Retrying in 1.3s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.4s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.5s...\n",
            "‚è≥ Rate limit hit. Retrying in 1.5s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.8s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.3s...\n",
            "‚úÖ Judge scores: [3.0, 3.0, 3.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn nest-asyncio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDI5VknaofAz",
        "outputId": "06093d30-5fca-4d49-b901-e615c9cfc73e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (0.123.10)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.38.0)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.50.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from fastapi) (2.12.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (4.15.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.0.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.51.0,>=0.40.0->fastapi) (4.12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.51.0,>=0.40.0->fastapi) (3.11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "\n",
        "nest_asyncio.apply()\n"
      ],
      "metadata": {
        "id": "zOVMhabtohhw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EvalRequest(BaseModel):\n",
        "    question: str\n",
        "    ground_truth: str\n",
        "    model: str = \"gpt-4o-mini\"\n",
        "\n",
        "class EvalResponse(BaseModel):\n",
        "    model: str\n",
        "    prediction: str\n",
        "    latency_sec: float\n",
        "    cost_usd: float\n",
        "    judge_score: float\n"
      ],
      "metadata": {
        "id": "mRmaRkJ5olog"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = FastAPI(title=\"LLM Evaluation & Cost Optimization API\")\n",
        "\n",
        "@app.post(\"/evaluate\", response_model=EvalResponse)\n",
        "def evaluate_llm(req: EvalRequest):\n",
        "\n",
        "    prediction, latency, cost = run_llm(req.model, req.question)\n",
        "\n",
        "    if req.ground_truth.lower() in prediction.lower():\n",
        "        judge_score = 5.0\n",
        "    elif prediction.strip():\n",
        "        judge_score = 3.0\n",
        "    else:\n",
        "        judge_score = 0.0\n",
        "\n",
        "    return EvalResponse(\n",
        "        model=req.model,\n",
        "        prediction=prediction,\n",
        "        latency_sec=round(latency, 3),\n",
        "        cost_usd=round(cost, 6),\n",
        "        judge_score=judge_score\n",
        "    )\n"
      ],
      "metadata": {
        "id": "NceK4ANkooto"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "\n",
        "def run():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "thread = threading.Thread(target=run)\n",
        "thread.start()\n"
      ],
      "metadata": {
        "id": "lLAD257LpDIG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "time.sleep(2)  # server start hone do\n",
        "\n",
        "url = \"http://localhost:8000/evaluate\"\n",
        "\n",
        "payload = {\n",
        "    \"question\": \"What is aspirin used for?\",\n",
        "    \"ground_truth\": \"pain relief\",\n",
        "    \"model\": \"gpt-4o-mini\"\n",
        "}\n",
        "\n",
        "response = requests.post(url, json=payload)\n",
        "print(response.json())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN1ZA-l1pFoA",
        "outputId": "23e4cead-8e7f-4b0a-f23b-6bac58d5110e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Rate limit hit. Retrying in 1.5s...\n",
            "‚è≥ Rate limit hit. Retrying in 2.3s...\n",
            "‚è≥ Rate limit hit. Retrying in 4.1s...\n",
            "INFO:     127.0.0.1:42380 - \"POST /evaluate HTTP/1.1\" 200 OK\n",
            "{'model': 'gpt-4o-mini', 'prediction': 'ERROR', 'latency_sec': 0.0, 'cost_usd': 0.0, 'judge_score': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zfLrSMMGpGn2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}